{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1-0 - Prelude"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start with the Kaggle Titanic dataset. The dataset is a list of Titanic passengers with features such as age, sex and class. We'll use these features to train a model, and use the model to predict whether or not each passenger survived. A more detailed treatment of the dataset can be found here:\n",
    "\n",
    "https://github.com/savarin/python_for_ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from time import time\n",
    "\n",
    "np.random.seed(1337)\n",
    "\n",
    "df = pd.read_csv('data/titanic.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex  Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male   22      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female   38      1   \n",
       "2                             Heikkinen, Miss. Laina  female   26      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female   35      1   \n",
       "4                           Allen, Mr. William Henry    male   35      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split the data 80-20 into training and test sets, fill in missing values and convert strings to numbers. In addition, we scale the data so that each column has mean 0 and standard deviation 1, and create one-hot vectors with the labels (analogous to dummy variables)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train = df.iloc[:712, :]\n",
    "\n",
    "df_train = df_train.drop(['Name', 'Ticket', 'Cabin', 'Embarked'], axis=1)\n",
    "\n",
    "age_mean = df_train['Age'].mean()\n",
    "df_train['Age'] = df_train['Age'].fillna(age_mean)\n",
    "df_train['Sex'] = df_train['Sex'].map({'female': 0, 'male': 1}).astype(int)\n",
    "\n",
    "features = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare']\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train = scaler.fit_transform(df_train[features].values)\n",
    "y_train = df_train['Survived'].values\n",
    "y_train_onehot = pd.get_dummies(df_train['Survived']).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_test = df.iloc[712:, :]\n",
    "\n",
    "df_test = df_test.drop(['Name', 'Ticket', 'Cabin', 'Embarked'], axis=1)\n",
    "\n",
    "df_test['Age'] = df_test['Age'].fillna(age_mean)\n",
    "df_test['Sex'] = df_test['Sex'].map({'female': 0, 'male': 1}).astype(int)\n",
    "\n",
    "X_test = scaler.transform(df_test[features].values)\n",
    "y_test = df_test['Survived'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " ## Benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a basis for comparison, we train a Random Forest model and record the accuracy on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 10\n",
      "building tree 2 of 10\n",
      "building tree 3 of 10\n",
      "building tree 4 of 10\n",
      "building tree 5 of 10\n",
      "building tree 6 of 10\n",
      "building tree 7 of 10\n",
      "building tree 8 of 10\n",
      "building tree 9 of 10\n",
      "building tree 10 of 10\n",
      "\n",
      "accuracy 0.832402234637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model = RandomForestClassifier(random_state=0, verbose=3)\n",
    "model = model.fit(X_train, y_train)\n",
    "\n",
    "y_prediction = model.predict(X_test)\n",
    "print \"\\naccuracy\", np.sum(y_prediction == y_test) / float(len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 1-layer Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the input, we have a vector of length 6 that represents each passenger's features. As an example, we consider the first passenger in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.83290956  0.74926865 -0.6178933   0.4436936  -0.47015218 -0.51933199]\n"
     ]
    }
   ],
   "source": [
    "print X_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the output, we want a vector of length 2 to represent the survival probabilities. A simple way to create this mapping is by using a 2x6 matrix. To start off, we generate a random matrix representing feature weights and apply the matrix to our input. We'll also add a bias term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.02620247  0.0158684   0.02781265  0.04593169  0.03210005  0.05183928]\n",
      " [ 0.02619429  0.09760853  0.07328146  0.01152742  0.03862751  0.06285012]]\n"
     ]
    }
   ],
   "source": [
    "W = np.random.rand(2, 6) / 10\n",
    "\n",
    "print W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.01250579  0.09835486]\n"
     ]
    }
   ],
   "source": [
    "b = np.random.rand(2,) / 10\n",
    "\n",
    "print b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.00740041  0.10234099]\n"
     ]
    }
   ],
   "source": [
    "result = np.dot(W, X_train[0]) + b\n",
    "\n",
    "print result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the output vector to sum to 1, we apply a softmax mapping. The first element would now represent the probability that the passenger did not survive, and the second element represents the probability the passenger survives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    return np.exp(x) / np.exp(x).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.47628267  0.52371733]\n"
     ]
    }
   ],
   "source": [
    "result = softmax(result)\n",
    "\n",
    "print result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then compare the output vector to the actual label. We would have a 'good' model if the probability for the correct label was close to 1, and a 'bad' one if it was close to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.  0.]\n"
     ]
    }
   ],
   "source": [
    "print y_train_onehot[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "label_index = np.argmax(y_train_onehot[0])\n",
    "\n",
    "print label_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.476282667565\n"
     ]
    }
   ],
   "source": [
    "print result[label_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define loss to be the negative logarithm of the probability of the correct label. Taking the logarithm penalizes the model for having a high probability associated with the wrong label. Here we have the loss associated with the first passenger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.741743761582\n"
     ]
    }
   ],
   "source": [
    "loss = -np.log(result[label_index])\n",
    "\n",
    "print loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now iterate through 1000 iterations for random values of W and b, and keep the pair which minimizes the average loss across all passengers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.685 accuracy 0.64 loop 0\n",
      "loss 0.683 accuracy 0.642 loop 1\n",
      "loss 0.681 accuracy 0.695 loop 5\n",
      "loss 0.675 accuracy 0.712 loop 8\n",
      "loss 0.673 accuracy 0.737 loop 44\n",
      "loss 0.661 accuracy 0.739 loop 47\n",
      "loss 0.658 accuracy 0.715 loop 297\n",
      "loss 0.652 accuracy 0.77 loop 514\n",
      "\n",
      "time taken 8.20281195641 seconds\n"
     ]
    }
   ],
   "source": [
    "min_loss = 1000\n",
    "best_weights = ()\n",
    "\n",
    "start = time()\n",
    "\n",
    "for i in xrange(1000):\n",
    "    W = np.random.rand(2, 6) / 10\n",
    "    b = np.random.rand(2,) / 10\n",
    "\n",
    "    scores = []\n",
    "    loss = 0\n",
    "    \n",
    "    for j in xrange(X_train.shape[0]):\n",
    "        result = np.dot(W, X_train[j]) + b\n",
    "        result = softmax(result)\n",
    "        scores.append(list(result))\n",
    "        \n",
    "        label_index = np.argmax(y_train_onehot[j])\n",
    "        loss += -np.log(result[label_index])\n",
    "\n",
    "    loss = loss / float(X_train.shape[0])\n",
    "    y_prediction = np.argmax(np.array(scores), axis=1)\n",
    "    accuracy = np.sum(y_prediction == y_train) / float(len(y_train))\n",
    "    \n",
    "    if loss < min_loss:\n",
    "        min_loss = loss\n",
    "        best_weights = (W, b)\n",
    "        print 'loss %s accuracy %s loop %s' % (round(loss, 3), round(accuracy, 3), i)\n",
    "\n",
    "print '\\ntime taken %s seconds' % str(time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.810055865922\n"
     ]
    }
   ],
   "source": [
    "W, b = best_weights\n",
    "scores = []\n",
    "\n",
    "for j in xrange(X_test.shape[0]):\n",
    "    result = np.dot(W, X_test[j]) + b\n",
    "    result = softmax(result)\n",
    "    scores.append(list(result))\n",
    "\n",
    "y_prediction = np.argmax(np.array(scores), axis=1)\n",
    "\n",
    "print \"accuracy\", np.sum(y_prediction == y_test) / float(len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each passenger, predictions for the test set were made by selecting the label with the highest probability. Despite the naÃ¯ve approach, we obtain a prediction accuracy of 81%!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 2-layer Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the 1-layer neural network, we had a 2x6 weight matrix. To create more degrees of freedom, we can introduce intermediary matrices or 'layers'. For example, instead of having a mapping from a vector of length 6 to a vector of length 2, we'll have two mappings - first from 6 to 100, followed by 100 to 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W_1 = np.random.rand(100, 6) / 10\n",
    "b_1 = np.random.rand(100,) / 10\n",
    "W_2 = np.random.rand(2, 100) / 10\n",
    "b_2 = np.random.rand(2,) / 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  3.85833932e-02   5.89968658e-02   5.25942066e-03   7.43856347e-02\n",
      "   1.14824434e-05   1.15027700e-01   3.68012540e-02   1.09882469e-01\n",
      "   9.05096555e-02   1.49145180e-01   4.92299995e-02   7.15835553e-02\n",
      "  -2.30711740e-02   8.81330372e-02   3.01255392e-03   1.08960642e-01\n",
      "   6.98125076e-02   5.54114438e-02   6.55790909e-02   8.08632209e-02\n",
      "   1.16684986e-01   1.64563621e-01   4.72968084e-02   6.44781150e-02\n",
      "   6.56903272e-02   3.71233636e-02   4.19944730e-02   1.65645958e-01\n",
      "   2.50494467e-02   1.94216742e-02   9.79751463e-02   3.02635378e-02\n",
      "   6.33219632e-04   8.73328656e-02   2.90520962e-02   9.43040144e-02\n",
      "   1.01387639e-01   6.37643873e-02   1.33784314e-01   8.22222654e-02\n",
      "   2.26342486e-01   1.60054641e-01   1.47240435e-01   7.64466972e-02\n",
      "   8.22117015e-02   8.62467846e-02  -3.58229687e-02   1.11880085e-01\n",
      "   1.65073281e-02   1.70666967e-01   7.91422152e-02   3.39914112e-02\n",
      "   1.22783230e-01   1.47883196e-01   1.37466479e-01   1.43118761e-01\n",
      "   2.83526009e-02   1.20861416e-01   1.39187762e-01   4.61296444e-02\n",
      "   4.60807014e-02   6.68309754e-02   6.13703935e-02   1.60373147e-01\n",
      "   9.32157216e-02   1.20221530e-01   8.43113509e-02   1.42663702e-01\n",
      "   5.05880323e-02   1.47941372e-01   9.85496936e-02   1.08617063e-01\n",
      "   1.18861700e-01   1.07656011e-01   2.06398630e-01   1.55910477e-01\n",
      "   1.27763183e-01   1.21465476e-01   1.02624677e-01   1.64056675e-01\n",
      "   3.27918035e-02   9.84728137e-02   1.34803581e-01   3.68592754e-02\n",
      "   1.80816523e-01   1.20420180e-01   1.14038740e-01  -2.26228527e-02\n",
      "   4.83168073e-02   1.47221411e-01   3.99776900e-02   1.29853123e-01\n",
      "  -2.60645036e-02   2.01247815e-01   4.25593860e-02   1.32287296e-01\n",
      "   1.59670750e-01  -9.21166064e-04   7.59365124e-02   6.66142678e-02]\n"
     ]
    }
   ],
   "source": [
    "result = np.dot(W_1, X_train[0]) + b_1\n",
    "\n",
    "print result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.46288862  0.44362129]\n"
     ]
    }
   ],
   "source": [
    "result = np.dot(W_2, result) + b_2\n",
    "\n",
    "print result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before we iterate through 1000 iterations of random values for W_1, b_1, W_2 and b_2, and keep the one which minimizes loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.682 accuracy 0.61 loop 0\n",
      "loss 0.679 accuracy 0.757 loop 5\n",
      "loss 0.678 accuracy 0.657 loop 8\n",
      "loss 0.675 accuracy 0.787 loop 15\n",
      "loss 0.67 accuracy 0.676 loop 29\n",
      "loss 0.668 accuracy 0.633 loop 333\n",
      "loss 0.663 accuracy 0.702 loop 362\n",
      "loss 0.663 accuracy 0.708 loop 590\n",
      "\n",
      "time taken 9.62385892868 seconds\n"
     ]
    }
   ],
   "source": [
    "min_loss = 1000\n",
    "best_weights = ()\n",
    "\n",
    "start = time()\n",
    "\n",
    "for i in xrange(1000):\n",
    "    W_1 = np.random.rand(100, 6) / 10\n",
    "    b_1 = np.random.rand(100,) / 10\n",
    "    W_2 = np.random.rand(2, 100) / 10\n",
    "    b_2 = np.random.rand(2,) / 10\n",
    "    \n",
    "    scores = []\n",
    "    loss = 0\n",
    "\n",
    "    for j in xrange(X_train.shape[0]):\n",
    "        result = np.dot(W_1, X_train[j]) + b_1\n",
    "        result = np.dot(W_2, result) + b_2\n",
    "        result = softmax(result)\n",
    "        scores.append(list(result))\n",
    "        \n",
    "        label_index = np.argmax(y_train_onehot[j])\n",
    "        loss += -np.log(result[label_index])\n",
    "\n",
    "    loss = loss / float(X_train.shape[0])\n",
    "    y_prediction = np.argmax(np.array(scores), axis=1)\n",
    "    accuracy = np.sum(y_prediction == y_train) / float(len(y_train))\n",
    "        \n",
    "    if loss < min_loss:\n",
    "        min_loss = loss\n",
    "        best_weights = (W_1, b_1, W_2, b_2)\n",
    "        print 'loss %s accuracy %s loop %s' % (round(loss, 3), round(accuracy, 3), i)\n",
    "\n",
    "print '\\ntime taken %s seconds' % str(time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.670391061453\n"
     ]
    }
   ],
   "source": [
    "W_1, b_1, W_2, b_2 = best_weights\n",
    "scores = []\n",
    "\n",
    "for j in xrange(X_test.shape[0]):\n",
    "    result = np.dot(W_1, X_test[j]) + b_1\n",
    "    result = np.dot(W_2, result) + b_2\n",
    "    result = softmax(result)\n",
    "    scores.append(list(result))\n",
    "    \n",
    "y_prediction = np.argmax(np.array(scores), axis=1)\n",
    "\n",
    "print \"accuracy\", np.sum(y_prediction == y_test) / float(len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite the greater degree of freedom, we get an accuracy score of only 67%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 3-layer Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We take this a step further by adding an additional layer, and similarly review model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.797 accuracy 0.287 loop 0\n",
      "loss 0.753 accuracy 0.296 loop 1\n",
      "loss 0.752 accuracy 0.285 loop 2\n",
      "loss 0.695 accuracy 0.39 loop 4\n",
      "loss 0.656 accuracy 0.705 loop 5\n",
      "loss 0.644 accuracy 0.709 loop 7\n",
      "loss 0.642 accuracy 0.705 loop 12\n",
      "loss 0.633 accuracy 0.709 loop 19\n",
      "loss 0.621 accuracy 0.721 loop 79\n",
      "loss 0.621 accuracy 0.715 loop 130\n",
      "loss 0.619 accuracy 0.728 loop 501\n",
      "loss 0.618 accuracy 0.715 loop 956\n",
      "\n",
      "time taken 13.7536799908 seconds\n"
     ]
    }
   ],
   "source": [
    "min_loss = 1000\n",
    "best_weights = ()\n",
    "\n",
    "start = time()\n",
    "\n",
    "for i in xrange(1000):\n",
    "    W_1 = np.random.rand(100, 6) / 10\n",
    "    b_1 = np.random.rand(100,) / 10\n",
    "    W_2 = np.random.rand(100, 100) / 10\n",
    "    b_2 = np.random.rand(100,) / 10\n",
    "    W_3 = np.random.rand(2, 100) / 10\n",
    "    b_3 = np.random.rand(2,) / 10\n",
    "    \n",
    "    scores = []\n",
    "    loss = 0\n",
    "\n",
    "    for j in xrange(X_train.shape[0]):\n",
    "        result = np.dot(W_1, X_train[j]) + b_1\n",
    "        result = np.dot(W_2, result) + b_2\n",
    "        result = np.dot(W_3, result) + b_3\n",
    "        result = softmax(result)\n",
    "        scores.append(list(result))\n",
    "        \n",
    "        label_index = np.argmax(y_train_onehot[j])\n",
    "        loss += -np.log(result[label_index])\n",
    "        \n",
    "    loss = loss / float(X_train.shape[0])\n",
    "    y_prediction = np.argmax(np.array(scores), axis=1)\n",
    "    accuracy = np.sum(y_prediction == y_train) / float(len(y_train))          \n",
    "        \n",
    "    if loss < min_loss:\n",
    "        min_loss = loss\n",
    "        best_weights = (W_1, b_1, W_2, b_2, W_3, b_3)\n",
    "        print 'loss %s accuracy %s loop %s' % (round(loss, 3), round(accuracy, 3), i)\n",
    "\n",
    "print '\\ntime taken %s seconds' % str(time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.703910614525\n"
     ]
    }
   ],
   "source": [
    "W_1, b_1, W_2, b_2, W_3, b_3 = best_weights\n",
    "scores = []\n",
    "\n",
    "for j in xrange(X_test.shape[0]):\n",
    "    result = np.dot(W_1, X_test[j]) + b_1\n",
    "    result = np.dot(W_2, result) + b_2\n",
    "    result = np.dot(W_3, result) + b_3    \n",
    "    result = softmax(result)\n",
    "    scores.append(list(result))\n",
    "    \n",
    "y_prediction = np.argmax(np.array(scores), axis=1)\n",
    "\n",
    "print \"accuracy\", np.sum(y_prediction == y_test) / float(len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run into the same problem of suboptimal model performance, but this is not unexpected given the simplistic approach taken for illustrative purposes. We look to build on this in the next section by taking a systematic approach to optimizing the weight matrices and biases."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
