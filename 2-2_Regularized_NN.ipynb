{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2-2 - Regularized Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we'll introduce two additional layers to our model. The first is called the rectified linear unit (hereafter, ReLU), which helps introduce non-linearity into the network. The second is called the dropout layer, which acts to regularize the network and prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from time import time\n",
    "\n",
    "np.random.seed(1337)\n",
    "\n",
    "df = pd.read_csv('data/mnist.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_train = df.iloc[:33600, :]\n",
    "\n",
    "X_train = df_train.iloc[:, 1:].values / 255.\n",
    "y_train = df_train['label'].values\n",
    "y_train_onehot = pd.get_dummies(df_train['label']).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_test = df.iloc[33600:, :]\n",
    "\n",
    "X_test = df_test.iloc[:, 1:].values / 255.\n",
    "y_test = df_test['label'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 10\n",
      "building tree 2 of 10\n",
      "building tree 3 of 10\n",
      "building tree 4 of 10\n",
      "building tree 5 of 10\n",
      "building tree 6 of 10\n",
      "building tree 7 of 10\n",
      "building tree 8 of 10\n",
      "building tree 9 of 10\n",
      "building tree 10 of 10\n",
      "\n",
      "accuracy 0.937857142857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    2.0s finished\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model = RandomForestClassifier(random_state=0, verbose=3)\n",
    "model = model.fit(X_train, df_train['label'].values)\n",
    "\n",
    "y_prediction = model.predict(X_test)\n",
    "print \"\\naccuracy\", np.sum(y_prediction == df_test['label'].values) / float(len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularized NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While matrix operations are linear, there could be a non-linear relationship between the features and the label. Introducing a ReLU layer, defined as f(x) = max(0, x), can help the model capture this interaction. ReLU is widely used as its simplicity allows for much faster training without a high cost to accuracy.\n",
    "\n",
    "The dropout layer can be thought of as a form of sampling, where output values are randomly set to zero by a pre-specified probability. This creates a more robust network as the process prevents interdependence, and as such the model is less likely to overfit on the training data. It is surprisingly effective, which has made it an active area of research.\n",
    "\n",
    "Following Andrej Karpathy's advice of \"don't be a hero\", the example shown here is from the Keras repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "33600/33600 [==============================] - 6s - loss: 0.8148 - acc: 0.7814     \n",
      "Epoch 2/10\n",
      "33600/33600 [==============================] - 6s - loss: 0.3841 - acc: 0.8886     \n",
      "Epoch 3/10\n",
      "33600/33600 [==============================] - 6s - loss: 0.3139 - acc: 0.9085     \n",
      "Epoch 4/10\n",
      "33600/33600 [==============================] - 7s - loss: 0.2741 - acc: 0.9196     \n",
      "Epoch 5/10\n",
      "33600/33600 [==============================] - 7s - loss: 0.2444 - acc: 0.9286     \n",
      "Epoch 6/10\n",
      "33600/33600 [==============================] - 7s - loss: 0.2184 - acc: 0.9361     \n",
      "Epoch 7/10\n",
      "33600/33600 [==============================] - 6s - loss: 0.1995 - acc: 0.9413     \n",
      "Epoch 8/10\n",
      "33600/33600 [==============================] - 7s - loss: 0.1840 - acc: 0.9456     \n",
      "Epoch 9/10\n",
      "33600/33600 [==============================] - 6s - loss: 0.1715 - acc: 0.9492     \n",
      "Epoch 10/10\n",
      "33600/33600 [==============================] - 7s - loss: 0.1565 - acc: 0.9536     \n",
      "\n",
      "time taken 69.9661490917 seconds\n"
     ]
    }
   ],
   "source": [
    "# https://github.com/fchollet/keras/blob/master/examples/mnist_mlp.py\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "\n",
    "start = time()\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=(784,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train_onehot)\n",
    "\n",
    "print '\\ntime taken %s seconds' % str(time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8288/8400 [============================>.] - ETA: 0s\n",
      "\n",
      "accuracy 0.957976190476\n"
     ]
    }
   ],
   "source": [
    "y_prediction = model.predict_classes(X_test)\n",
    "print \"\\n\\naccuracy\", np.sum(y_prediction == y_test) / float(len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introducing ReLU and dropout layers have enabled the model to outperform the benchmark by 2%! In the next section, we introduce new layers that take advantage of the 2D structure of the image to further improve model performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
