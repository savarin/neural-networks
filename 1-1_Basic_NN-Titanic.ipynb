{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1-1 - Basic Neural Network - Titanic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous section, we simply iterated through randomly-generated matrices and chose the best-performing one. We build on this approach by reducing loss in a systematic way via stochastic gradient descent. In particular, we'll be using TensorFlow, an open source library developed by Google, and Keras, a high-level wrapper on top of TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from time import time\n",
    "\n",
    "np.random.seed(1337)\n",
    "\n",
    "df = pd.read_csv('data/titanic.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_train = df.iloc[:712, :]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "features = ['Class', 'Sex', 'Age', 'Fare']\n",
    "\n",
    "X_train = scaler.fit_transform(df_train[features].values)\n",
    "y_train = df_train['Survived'].values\n",
    "y_train_onehot = pd.get_dummies(df_train['Survived']).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_test = df.iloc[712:, :]\n",
    "\n",
    "X_test = scaler.transform(df_test[features].values)\n",
    "y_test = df_test['Survived'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 10\n",
      "building tree 2 of 10\n",
      "building tree 3 of 10\n",
      "building tree 4 of 10\n",
      "building tree 5 of 10\n",
      "building tree 6 of 10\n",
      "building tree 7 of 10\n",
      "building tree 8 of 10\n",
      "building tree 9 of 10\n",
      "building tree 10 of 10\n",
      "\n",
      "accuracy 0.810055865922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model = RandomForestClassifier(random_state=0, verbose=3)\n",
    "model = model.fit(X_train, df_train['Survived'].values)\n",
    "\n",
    "y_prediction = model.predict(X_test)\n",
    "print \"\\naccuracy\", np.sum(y_prediction == y_test) / float(len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 1-layer Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of generating a linear stack of layers with Numpy, we'll be implementing our model using Keras. We initialize our model, add a layer that inputs vectors of length 4 and outputs vectors of length 2, and finally add a softmax layer. We configure the learning process in the compilation step by specifying the optimizer, loss function and performance metric.\n",
    "\n",
    "Stochastic gradient descent acts by changing the weights gradually in the 'direction' that decreases the average loss. In other words, a particular weight would be increased if acts to decrease loss, or the weight decreased if it acts to increase loss. TensorFlow does the heavy-lifting by efficiently handling these numerical computations under the hood. A simple example of stochastic gradient descent is illustrated in the Appendix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "712/712 [==============================] - 0s - loss: 0.6138 - acc: 0.7261     \n",
      "Epoch 2/10\n",
      "712/712 [==============================] - 0s - loss: 0.5947 - acc: 0.7360     \n",
      "Epoch 3/10\n",
      "712/712 [==============================] - 0s - loss: 0.5791 - acc: 0.7402     \n",
      "Epoch 4/10\n",
      "712/712 [==============================] - 0s - loss: 0.5651 - acc: 0.7472     \n",
      "Epoch 5/10\n",
      "712/712 [==============================] - 0s - loss: 0.5544 - acc: 0.7598     \n",
      "Epoch 6/10\n",
      "712/712 [==============================] - 0s - loss: 0.5442 - acc: 0.7626     \n",
      "Epoch 7/10\n",
      "712/712 [==============================] - 0s - loss: 0.5361 - acc: 0.7626     \n",
      "Epoch 8/10\n",
      "712/712 [==============================] - 0s - loss: 0.5295 - acc: 0.7640     \n",
      "Epoch 9/10\n",
      "712/712 [==============================] - 0s - loss: 0.5225 - acc: 0.7654     \n",
      "Epoch 10/10\n",
      "712/712 [==============================] - 0s - loss: 0.5170 - acc: 0.7739     \n",
      "\n",
      "time taken 0.727828979492 seconds\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "\n",
    "start = time()\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(input_dim=4, output_dim=2))\n",
    "model.add(Activation(\"softmax\"))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train_onehot)\n",
    "\n",
    "print '\\ntime taken %s seconds' % str(time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 32/179 [====>.........................] - ETA: 0s\n",
      "\n",
      "accuracy 0.782122905028\n"
     ]
    }
   ],
   "source": [
    "y_prediction = model.predict_classes(X_test)\n",
    "print \"\\n\\naccuracy\", np.sum(y_prediction == y_test) / float(len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that the loss reduces systematically as the model 'learns' from the data. The rate of loss reduction, however, seems to indicate that loss could be further reduced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 2-layer Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "712/712 [==============================] - 0s - loss: 0.5477 - acc: 0.7612     \n",
      "Epoch 2/10\n",
      "712/712 [==============================] - 0s - loss: 0.5183 - acc: 0.7711     \n",
      "Epoch 3/10\n",
      "712/712 [==============================] - 0s - loss: 0.5014 - acc: 0.7753     \n",
      "Epoch 4/10\n",
      "712/712 [==============================] - 0s - loss: 0.4921 - acc: 0.7739     \n",
      "Epoch 5/10\n",
      "712/712 [==============================] - 0s - loss: 0.4857 - acc: 0.7781     \n",
      "Epoch 6/10\n",
      "712/712 [==============================] - 0s - loss: 0.4813 - acc: 0.7767     \n",
      "Epoch 7/10\n",
      "712/712 [==============================] - 0s - loss: 0.4774 - acc: 0.7753     \n",
      "Epoch 8/10\n",
      "712/712 [==============================] - 0s - loss: 0.4749 - acc: 0.7767     \n",
      "Epoch 9/10\n",
      "712/712 [==============================] - 0s - loss: 0.4725 - acc: 0.7767     \n",
      "Epoch 10/10\n",
      "712/712 [==============================] - 0s - loss: 0.4710 - acc: 0.7753     \n",
      "\n",
      "time taken 0.797566175461 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(input_dim=4, output_dim=100))\n",
    "model.add(Dense(output_dim=2))\n",
    "model.add(Activation(\"softmax\"))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train_onehot)\n",
    "\n",
    "print '\\ntime taken %s seconds' % str(time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 32/179 [====>.........................] - ETA: 0s\n",
      "\n",
      "accuracy 0.810055865922\n"
     ]
    }
   ],
   "source": [
    "y_prediction = model.predict_classes(X_test)\n",
    "print \"\\n\\naccuracy\", np.sum(y_prediction == y_test) / float(len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss reduction 'flattens out' more compared to the 1-layer example, and the accuracy improves to 81%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 3-layer Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "712/712 [==============================] - 0s - loss: 0.5707 - acc: 0.7346     \n",
      "Epoch 2/10\n",
      "712/712 [==============================] - 0s - loss: 0.4980 - acc: 0.7809     \n",
      "Epoch 3/10\n",
      "712/712 [==============================] - 0s - loss: 0.4772 - acc: 0.7795     \n",
      "Epoch 4/10\n",
      "712/712 [==============================] - 0s - loss: 0.4704 - acc: 0.7823     \n",
      "Epoch 5/10\n",
      "712/712 [==============================] - 0s - loss: 0.4676 - acc: 0.7809     \n",
      "Epoch 6/10\n",
      "712/712 [==============================] - 0s - loss: 0.4664 - acc: 0.7823     \n",
      "Epoch 7/10\n",
      "712/712 [==============================] - 0s - loss: 0.4653 - acc: 0.7823     \n",
      "Epoch 8/10\n",
      "712/712 [==============================] - 0s - loss: 0.4649 - acc: 0.7851     \n",
      "Epoch 9/10\n",
      "712/712 [==============================] - 0s - loss: 0.4642 - acc: 0.7851     \n",
      "Epoch 10/10\n",
      "712/712 [==============================] - 0s - loss: 0.4649 - acc: 0.7865     \n",
      "\n",
      "time taken 1.09251308441 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(input_dim=4, output_dim=100))\n",
    "model.add(Dense(output_dim=100))\n",
    "model.add(Dense(output_dim=2))\n",
    "model.add(Activation(\"softmax\"))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train_onehot)\n",
    "\n",
    "print '\\ntime taken %s seconds' % str(time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 32/179 [====>.........................] - ETA: 0s\n",
      "\n",
      "accuracy 0.815642458101\n"
     ]
    }
   ],
   "source": [
    "y_prediction = model.predict_classes(X_test)\n",
    "print \"\\n\\naccuracy\", np.sum(y_prediction == y_test) / float(len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we're able to reduce loss on the training set a little further, the best performance obtained is merely comparable to our benchmark. Since the dataset is small, there isn't as much for the model to 'learn' from (or for that matter, predict on). We'll apply techniques developed so far on a much larger dataset in the next section."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
